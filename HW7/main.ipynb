{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from abc import ABC\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    torch.cuda.manual_seed(666)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "torch.manual_seed(666)\n",
    "random.seed(666)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_data = json.load(open('hw7_train.json', 'r', encoding='utf-8'))\n",
    "dev_data = json.load(open('hw7_dev.json', 'r', encoding='utf-8'))\n",
    "test_data = json.load(open('hw7_test.json', 'r', encoding='utf-8'))\n",
    "\n",
    "train_questions = train_data['questions']\n",
    "train_paragraphs = train_data['paragraphs']\n",
    "\n",
    "dev_questions = dev_data['questions']\n",
    "dev_paragraphs = dev_data['paragraphs']\n",
    "\n",
    "test_questions = test_data['questions']\n",
    "test_paragraphs = test_data['paragraphs']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-base-chinese')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (570 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_question = tokenizer([text['question_text'].replace(' ', '') for text in train_questions], add_special_tokens=False)\n",
    "tokenized_dev_question = tokenizer([text['question_text'].replace(' ', '') for text in dev_questions], add_special_tokens=False)\n",
    "tokenized_test_question = tokenizer([text['question_text'].replace(' ', '') for text in test_questions], add_special_tokens=False)\n",
    "tokenized_train_paragraphs = tokenizer(list(map(lambda x : x.replace(' ', ''), train_paragraphs)), add_special_tokens=False)\n",
    "tokenized_dev_paragraphs = tokenizer(list(map(lambda x : x.replace(' ', ''), dev_paragraphs)), add_special_tokens=False)\n",
    "tokenized_test_paragraphs = tokenizer(list(map(lambda x : x.replace(' ', ''), test_paragraphs)), add_special_tokens=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# print(len(train_paragraphs))\n",
    "# print(max([len(para) for para in train_paragraphs]))\n",
    "# print(max([len(q['question_text']) for q in train_questions]))\n",
    "# print(list(filter(lambda x : len(x) >= 221, [q['question_text'] for q in train_questions])))\n",
    "# len(tokenized_train_paragraphs['input_ids'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, mode='train'):\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.questions = train_questions\n",
    "            self.tokenized_questions = tokenized_train_question\n",
    "            self.paragraphs = tokenized_train_paragraphs\n",
    "        elif self.mode == 'dev':\n",
    "            self.questions = dev_questions\n",
    "            self.tokenized_questions = tokenized_dev_question\n",
    "            self.paragraphs = tokenized_dev_paragraphs\n",
    "        else:\n",
    "            self.questions = test_questions\n",
    "            self.tokenized_questions = tokenized_test_question\n",
    "            self.paragraphs = tokenized_test_paragraphs\n",
    "        self.max_paragraph_len = 150\n",
    "        self.max_question_len = 30\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        question = self.questions[index]\n",
    "        tokenized_paragraph = self.paragraphs[question['paragraph_id']]\n",
    "        tokenized_question_text = self.tokenized_questions[index]\n",
    "\n",
    "        answer = question['answer_text']\n",
    "        input_ids_question = [101] + self.tokenized_questions[index].ids[0:self.max_question_len] + [102]\n",
    "        if self.mode == 'train':\n",
    "            start_pos = question['answer_start']\n",
    "            end_pos = question['answer_end']\n",
    "             # mid = (start_pos + end_pos) // 2\n",
    "        # if len(tokenized_paragraph) > self.max_paragraph_len:\n",
    "        #     para_start = max(0, mid - self.max_paragraph_len // 2)\n",
    "        #     para_end = min(para_start + self.max_paragraph_len, len(tokenized_paragraph.ids))\n",
    "        #     para_token_start = tokenized_paragraph.char_to_token(para_start)\n",
    "        #     para_token_end = tokenized_paragraph.char_to_token(para_end)\n",
    "            answer_start_in_token = tokenized_paragraph.char_to_token(start_pos)\n",
    "            answer_end_in_token = tokenized_paragraph.char_to_token(end_pos)\n",
    "            if answer_start_in_token is None or answer_end_in_token is None:\n",
    "                answer_start_in_token = start_pos\n",
    "                answer_end_in_token = end_pos\n",
    "            mid = (answer_end_in_token + answer_start_in_token) // 2\n",
    "            para_start = max(0, min(mid - self.max_question_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))\n",
    "            para_end = para_start + self.max_paragraph_len\n",
    "            input_ids_paragraph = tokenized_paragraph.ids[para_start : para_end] + [102]\n",
    "\n",
    "            # input_ids_paragraph = tokenized_paragraph.ids[para_token_start : para_token_end] + [102]\n",
    "            # if len(input_ids_paragraph) == 431:\n",
    "            #     print(tokenizer.decode(tokenized_paragraph.ids))\n",
    "            #     print('para_start = {0}, para_end = {1}'.format(para_start, para_end))\n",
    "            #     print('para_token_start = {0}, end = {1}'.format(para_token_start, para_token_end))\n",
    "            # answer_start_in_token = len(input_ids_question) + start_pos - para_start\n",
    "            # answer_end_in_token = len(input_ids_question) + end_pos - para_start\n",
    "            answer_start_in_token += len(input_ids_question) - para_start\n",
    "            answer_end_in_token += len(input_ids_question) - para_start\n",
    "\n",
    "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
    "            return torch.tensor(input_ids), torch.tensor(token_type_ids),torch.tensor(attention_mask), answer_start_in_token, answer_end_in_token\n",
    "\n",
    "        else:\n",
    "            input_ids_paragraph_list = []\n",
    "            m = len(tokenized_paragraph) // self.max_paragraph_len  + 1\n",
    "            for i in range(m):\n",
    "                input_ids_paragraph_list.append(tokenized_paragraph.ids[self.max_paragraph_len * i : min(self.max_paragraph_len * (i + 1), len(tokenized_paragraph))] + [102])\n",
    "            token_type_ids_list = []\n",
    "            attention_mask_list = []\n",
    "            input_ids_list = []\n",
    "            for p in input_ids_paragraph_list:\n",
    "                input_ids, token_type, attention_mask = self.padding(input_ids_question, p)\n",
    "                input_ids_list.append(input_ids)\n",
    "                token_type_ids_list.append(token_type)\n",
    "                attention_mask_list.append(attention_mask)\n",
    "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n",
    "\n",
    "    def padding(self, q, p):\n",
    "        padding_len = self.max_paragraph_len + self.max_question_len + 3 - len(q) - len(p)\n",
    "        # if padding_len < 0:\n",
    "        #     print(padding_len, len(q), len(p))\n",
    "        input_ids = q + p + [0] * padding_len\n",
    "        token_type_ids = [0] * len(q) + [1] * len(p) + [0] * padding_len\n",
    "        attention_mask = [1] * (len(q) + len(p)) + [0] * padding_len\n",
    "        return input_ids, token_type_ids, attention_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_set = MyDataset('train')\n",
    "dev_set = MyDataset('dev')\n",
    "test_set = MyDataset('test')\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "dev_loader = DataLoader(dev_set, batch_size=1)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# len(list(train_loader)[27])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "config = {'lr': 0.0001, 'epoch': 1}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def evaluate(data, output, index, mode='dev'):\n",
    "    if mode == 'dev':\n",
    "        questions = dev_questions\n",
    "    else:\n",
    "        questions = test_questions\n",
    "\n",
    "    question = questions[index]\n",
    "    answer_text = question['answer_text']\n",
    "    start_pos = torch.argmax(output.start_logits, dim=1)\n",
    "    end_pos = torch.argmax(output.end_logits, dim=1)\n",
    "    sum = float('-inf')\n",
    "    n = 0\n",
    "    for i in range(output.start_logits.shape[0]):\n",
    "        if sum < output.start_logits[i][start_pos[i]] + output.end_logits[i][end_pos[i]]:\n",
    "            sum = output.start_logits[i][start_pos[i]] + output.end_logits[i][end_pos[i]]\n",
    "            n = i\n",
    "    # print(output.start_logits.shape[0], data[0].shape)\n",
    "    # print(data[0])\n",
    "    pred_text = tokenizer.decode(data[0][0][n][start_pos[n] : end_pos[n] + 1]) # plus 1 is important here\n",
    "    if pred_text.replace(' ', '') == answer_text:\n",
    "        return 1, pred_text.replace(' ', '')\n",
    "    else:\n",
    "        return 0, pred_text.replace(' ', '')\n",
    "    # answer_start = question['answer_start']\n",
    "    # answer_end = question['answer']\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.optimizer = torch.optim.AdamW(params=self.model.parameters(), lr=self.config['lr'])\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        for epoch in range(self.config['epoch']):\n",
    "            progress_train = tqdm(train_loader)\n",
    "            progress_train.set_description('epoch {0}, training'.format(epoch+1))\n",
    "            steps = 0\n",
    "            acc = 0\n",
    "            accumulated_acc = 0\n",
    "            accumulated_loss = 0\n",
    "            for data in progress_train:\n",
    "                data = [i.to(device) for i in data]\n",
    "                steps += 1\n",
    "                output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n",
    "                loss = output.loss\n",
    "                start_pos = torch.argmax(output.start_logits, dim=1)\n",
    "                end_pos = torch.argmax(output.end_logits, dim=1)\n",
    "                acc = torch.mean(((start_pos == data[3]) & (end_pos == data[4])).float(), dim=0).item()\n",
    "                progress_train.set_postfix(loss=loss, acc=acc)\n",
    "                accumulated_acc += acc\n",
    "                accumulated_loss += loss.item()\n",
    "                output.loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            print('Average loss = {0}, Average acc = {1}'.format(accumulated_loss / steps, accumulated_acc / steps))\n",
    "\n",
    "            ###########  validation   #################\n",
    "            model.eval()\n",
    "            progress_dev = tqdm(dev_loader)\n",
    "            progress_dev.set_description('epoch {0}, validating'.format(epoch+1))\n",
    "            with torch.no_grad():\n",
    "                dev_acc = 0\n",
    "                for i, data in enumerate(progress_dev):\n",
    "                    # data = list(map(lambda x : list(map(lambda y : y.to(device), x)), data))\n",
    "\n",
    "\n",
    "                    # batch size of dev is 1, after squeezing dim 0, the number of split paragraph becomes dim 0\n",
    "                    output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device), attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "                    dev_acc += evaluate(data, output, i, 'dev')[0]\n",
    "                print('epoch {0}, dev acc = {1}'.format(epoch, dev_acc / len(dev_loader)))\n",
    "            model.train()\n",
    "            model.save_pretrained('./saved_model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        progress_test = tqdm(test_loader)\n",
    "        progress_test.set_description('predicting')\n",
    "\n",
    "        for i, data in enumerate(progress_test):\n",
    "            # data = list(map(lambda x : list(map(lambda y : y.to(device), x)), data))\n",
    "\n",
    "            output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device), attention_mask=data[2].squeeze(dim=0).to(device))\n",
    "            result.append(evaluate(data, output, i, 'test')[1])\n",
    "    with open('result.csv', 'w') as f:\n",
    "        f.write(\"ID,Answer\\n\")\n",
    "        # writer = csv.writer(f)\n",
    "        #   writer.writerow(['ID', 'Answer'])\n",
    "        for i, test_question in enumerate(test_questions):\n",
    "        # Replace commas in answers with empty strings (since csv is separated by comma)\n",
    "        # Answers in kaggle are processed in the same way\n",
    "            f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n",
    "\n",
    "    print(f\"Completed!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/496 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23e7f5fd838a4cd5b94b962697af2b58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss = 0.9106101567947096, Average acc = 0.6891822077093586\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4131 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03fcca09723841b89dc31afc47095153"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, dev acc = 0.5039941902687001\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4957 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6e9e5e15da04dbb95174b785d0712de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config)\n",
    "trainer.train()\n",
    "infer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 183])\n",
      "----------------------------------\n",
      "(tensor([[  101,  9246,  8156,  2399,  4635,  5909,  3136,  2530,  6629,  5412,\n",
      "          4638,  5178,  3362,  3221,   136,   102,  1039,  3308,  2527,  3309,\n",
      "          8024,  4294,  1162,  3221,  9146,  8129,  2399,   807,   704,  2527,\n",
      "          3309,  5635, 12809,  2399,   807,  3309,  7279,  8024,   746,  3197,\n",
      "           510,  4602,  4554,  5645,  3717,  4134,  3229,  2382,  4634,  4495,\n",
      "          8024,  7941,  3777,  1765,  1281,  3717,  2642,  2215,  1071,  1713,\n",
      "          7028,  8024,  5735,   809,  3644,   807,   704,  1751,  4374,  3308,\n",
      "          4638,  3613,  3149,   868,  3683,  6733,  8024,  4912,  4031,  2398,\n",
      "          1772,   129,   119,   129,  2399,   671,  3613,  8024,  1060,  2129,\n",
      "          4158,   124,   119,   126,  2399,  8024,  1039,   807,  4158,   122,\n",
      "           119,   127,  2399,  8024,  3209,   510,  3926,  1060,   807,  1772,\n",
      "          4158,   123,   119,   129,  2399,   511,  5645,  3634,  1398,  3229,\n",
      "          8024,  1039,  2455,   679,  3174,  3119,  1357,  1392,  4934,  6548,\n",
      "          4922,  8024,   886,  4636,  1998,  4638,  4495,  3833,  3291,  1217,\n",
      "          5681,  5736,  8024,   886,  2533,  4635,  5909,  3136,  6852,  4041,\n",
      "          3837,  6121,  8024,   699,  2768,  4158,  2205,  2834,  1039,  2455,\n",
      "          4638,  1248,  1213,   511,  3193,  1762,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  9246,  8156,  2399,  4635,  5909,  3136,  2530,  6629,  5412,\n",
      "          4638,  5178,  3362,  3221,   136,   102,  9306,  8157,  2399,  2218,\n",
      "          4634,  4495,  6882,  3777,  1298,  6635,   682,  2447,   510,  6958,\n",
      "          5835,  5958,  7526,  2206,  4638,  3636,  6172,  6629,   752,   511,\n",
      "          9246,  8156,  2399,  3736,  6205,  6145,  2336,  2510,  1469,  2213,\n",
      "           510,  1453,  2094,  3200,  5023,  4635,  5909,  3136,  2530,  6629,\n",
      "          5412,  1927,  3134,  8024,  2510,  1469,  2213,  6845,  5635,  3917,\n",
      "          6205,   511, 12809,  2399,  1039,  2455,   678,   808,  6365,  3291,\n",
      "          7045,  3791,  8024,  7139,  6863,   519,  5635,  3633,  6858,  2188,\n",
      "           520,  7092,  8024,   699,  1920,  7030,  4634,  6121,  3173,   519,\n",
      "           704,  5186,  1039,  2188,   769,  7045,   520,  8024,  2206,  5636,\n",
      "          4289,  1019,  6813,  6862,   677,  4039,   511,  7392,  2399,  1039,\n",
      "          2669,  2134,  3836,  6537,  7798,  3780,  7941,  3777,  8024,  3617,\n",
      "          3645,  3125,  6887,  8024,  1240,  4500,  3696,   829,  1282,   758,\n",
      "          5857,  8024,  1894,  1070,   753,  5857,   511,  5445,  2135,  1401,\n",
      "           733,  3582,  3145,  6266,  1239,  5164,  8024,  6863,  2768,   679,\n",
      "          4021,   511,  4635,  5909,  3136,  7674,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  9246,  8156,  2399,  4635,  5909,  3136,  2530,  6629,  5412,\n",
      "          4638,  5178,  3362,  3221,   136,   102,  7526,  7502,  2255,  4997,\n",
      "           510,  1208,  4886,  6858,  5023,   782,  3748,  2137,  1762,   126,\n",
      "          3299,  4372,  3136,  4707,  6629,   752,  8024,   852,   752,  3824,\n",
      "          8024,  7502,  2255,  4997,  6158,  2936,  3669,   511,  1208,  4886,\n",
      "          6858,  1086,  4989,  7502,  2255,  4997,   722,  2094,  7502,  3360,\n",
      "          1051,  3669,  1139,  7028,  1752,  8024,  2900,  7502,  2255,  4997,\n",
      "          4158,  2129,  2551,  2134,  1061,   686,  2113,  8024,  2802,  1139,\n",
      "           519,  2541,  2129,   520,  3186,  5998,  8024,   809,  5148,  2353,\n",
      "          4158,  3560,  6290,   511,  1071,  2527,  6958,  2094,  5646,  3176,\n",
      "          2128,  2551,  4090,  2336,  6629,   752,  8024,  5698,  7937,  3330,\n",
      "          5023,   782,  1304,  7526,  2528,  2336,  8024,  3634,  4158,  3346,\n",
      "          5143,  5148,  2353,  6725,   511,  6205,  5143,  5148,  2353,  6725,\n",
      "          3175,  7481,  8024,  2510,  4453,  4373,   510,  6966,  3249,  1245,\n",
      "          5645,  2528,  1904,  6740,  1762,  3959,  1266,   100,  2336,  6629,\n",
      "           752,  8024,  1751,  5998,  1921,  2130,   511,  5148,  2353,  6725,\n",
      "          1248,  1213,  6881,  2357,  3777,  1298,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0],\n",
      "        [  101,  9246,  8156,  2399,  4635,  5909,  3136,  2530,  6629,  5412,\n",
      "          4638,  5178,  3362,  3221,   136,   102,  3736,  1266,   510,  3736,\n",
      "          1298,   510,  1060,  3959,  5645,  1724,  2335,  5023,  1765,  8024,\n",
      "          6917,  3300,  7478,  5148,  2353,  6725,  4638,  2484,  1894,  6296,\n",
      "          5023,  6956,  4638,  6629,   752,  8024,  3696,  6365,  2999,  7274,\n",
      "          1039,  3308,  3994,   767,  4638,  2415,  2391,   511,  3176,  1039,\n",
      "          3314,  3696,  6365,  3309,  7279,  8024,  1894,   782,  1914,   679,\n",
      "          2244,  1347,  1217,  1361,  6725,  8024,  1361,  6725,   738,  2523,\n",
      "          2208,  1164,  4500,  1894,   782,   511,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]]), tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n"
     ]
    }
   ],
   "source": [
    "print(list(dev_loader)[1][1].squeeze(dim=0).shape)\n",
    "print('----------------------------------')\n",
    "print(dev_set[1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}